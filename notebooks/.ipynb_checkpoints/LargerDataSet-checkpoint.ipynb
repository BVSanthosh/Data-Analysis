{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Larger Data Set Notes\n",
    "\n",
    "For the sake of consistency, this notebook also contains the report however they have not been changed to match the new data if some comments seem out of place, that is why.\n",
    "\n",
    "This dataset was found by using TAGS 6.1 to search Twitter for a variety of Hashtags, users, etc (such as #Bitcoin, #Russia, etc). This was due its export limit being 18,000 Tweets at a time from at most a week away as to get a larger data set than the original, we had to combine many searches.\n",
    "\n",
    "We also had to remove the user_location column to match the provided data's format.\n",
    "\n",
    "One notable issue is the Timeline of Activity graph became quite warped in its plotting, unfortunately we did not have time to investigate this further.\n",
    "\n",
    "A benefit of this is it resulted in a far more varied data set which tests the program's adaptability even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "For this practical the objective was to perform data analysis using the tools for data analytics and visualisation provided by Python, as well as producing reproducible research using Jupyter notebooks. The dataset to be analysed consists of tweets on the landing of the Rosetta’s Philae lander on the comet 67P.\n",
    "\n",
    "Our repository link is: https://kb267.hg.cs.st-andrews.ac.uk/DataAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Functionality\n",
    "\n",
    "The following functionalities have been implemented in our program:\n",
    "- Basic Functionalities:\n",
    "    - Refinement of the original dataset by removing duplicates and formatting the data as required\n",
    "    - Provides an executable to automate the above functionality\n",
    "    - Includes unit tests to check the auxiliary functions, ensuring the robustness of the code\n",
    "    - Performs the following descriptive analysis:\n",
    "        - calculate the total number of tweets, retweets and replies \n",
    "        - calculate the number of different users tweeting\n",
    "        - calculate the average number of tweets, retweets and replies sent by a user\n",
    "        - identify most popular hashtags\n",
    "    - Produces the following visualisations: \n",
    "        - A structure of the dataset which includes tweets, retweets and replies\n",
    "        - A timeline of the tweet activities\n",
    "        - A word cloud of all other hashtags used in the tweets\n",
    "    - Includes the Jupyter notebook to re-run the analysis \n",
    "    - Includes an executable to regenerate all images and save them in a subdirectory\n",
    "- Additional Requirements:\n",
    "    - Shows the applications used to send tweets\n",
    "    - Extends the descriptive analysis, including providing accompanying visuals  \n",
    "    - Analyses the activity of individual users over the given period\n",
    "    - Analyses the interactions between users using the network library and produces visuals \n",
    "    - Includes interactive visualisations by providing features such as a slider to control the parameters or inspection of individual nodes\n",
    "    - Analysis of another much larger dataset\n",
    "    \n",
    "Reasons for and details of additional analysis will be found below alongside their results and outputs.\n",
    "\n",
    "The larger dataset analysis is found in its own Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown of Code between Notebooks and .py Files\n",
    "\n",
    "In this practical, we decided to have the majority of our Python code (including method and class definitions) in .py files. This then left the calling of executable files and visualization methods to the notebook itself.\n",
    "\n",
    "We decided to do this as it reduced the amount of code bulk present in our notebook, allowing for greater focus on the results of analysis and the output visuals. \n",
    "\n",
    "A breakdown of each executable file's purpose and can be found before its running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Known Problems in our Solution\n",
    "\n",
    "### Slow Loading of NetworkX Graphs\n",
    "\n",
    "Due to the large amount of computation and memory required to generate the NetworkX Graphs, it can take a long time to generate for larger sample sizes. This is because of the inherent deficiencies of network x and matplotlib as they are implemented purely in Python, which is known to be in general substantially slower than C/C++[1]. An alternative solution may be to use graph_viz[2] instead of the draw function when adding nodes and edges, or to use other technologies which do the same. For example, graph-tool[3] or igraph can be used.\n",
    "\n",
    "### Hard to Read Figures due to Concentrated Data\n",
    "\n",
    "Due to a large amount of the activity in the dataset being centralised around a few specific Users, Hashtags, time, etc. there are frequently graphs featuring one specific value being far larger than the others which can make them difficult to read however as this is the nature of the data, we could not solve this.\n",
    "\n",
    "### Interactive Visuals Disappearing on Refresh\n",
    "\n",
    "Sometimes on reloading the notebook, the interactive visuals can disappear and be replaced by text. This can be resolved by rerunning the generator commands.\n",
    "\n",
    "### Slow processing of the top n countries tweeted from\n",
    "\n",
    "Due to the limitations of the API provided by TomTom technologies, reverse geo coordinate search would take around 1 minutes and 20 seconds for the approximately 400 coordinates demonstrated in the map. The API limits the number of http requests for a single key to 5 requests per second[4] (400/5 = 80 seconds).\n",
    "\n",
    "### The Map API limits the number of daily requests to 50000 requests[5]\n",
    "\n",
    "### Count Map not Displaying Correctly in PDF\n",
    "\n",
    "When exporting the notebook to PDF using Print, the marks on the map do not display correctly which can make it harder to read than in the notebook itself. The numbers are still present, their backing however isn't.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "### Unit Tests\n",
    "\n",
    "Python unit tests were focused on auxiliary methods used in data refinement. For each, the result of running the refinement method on a test data frame and a premade correct dataset were compared for consistency.\n",
    "\n",
    "Helper methods were also tested to ensure they were functioning correctly as they are used throughout the refinement process.\n",
    "\n",
    "These were the focus specifically because they allowed us to ensure the refinement process was valid throughout, giving us greater confidence in the analysis results that followed.\n",
    "\n",
    "The Unit Tests can be run below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../code/Tests.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Testing\n",
    "\n",
    "Manual testing was focused on cases related to arguments when running the .py files and their methods or any other values the user can modify in the notebook as this is the main form of variances in the program's running and thus when most errors can occur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Test | Reason | Process | Expected Result | Result | \n",
    "| :- | :- | :- | :- | :- |\n",
    "| Refining already correctly formatted data | Testing re-refining a file doesn’t cause it to save again. | Calling refineData.py on a refined file. | No new file is saved | The file runs and saves no new file. | \n",
    "| Analysing refined data | Testing the data analysis functions correctly | Calling analyseData.py on a refined file. | A full output of analysis is returned. | The correct output is printed and allows for other visuals to be drawn. | \n",
    "| Analysing unrefined data | Testing the data analysis error handling functions correctly. | Calling analyseData.py on an unrefined file. | An appropriate error is returned. | The correct error is printed. | \n",
    "| Network analysing refined data | Testing the networkx-based analysis functions correctly. | Calling networkAnalysis.py on a refined file. | Refinement is run, allowing a graph to be drawn. | The correct graph is able to be drawn. | \n",
    "| Network analysing unrefined data | Testing the networkx-based analysis error handling functions correctly. | Calling networkAnalysis.py on an unrefined file. | An appropriate error is returned. | The correct error is printed. | \n",
    "| User analysing refined data | Testing the user-based analysis functions correctly. | Calling userActivity.py on a refined file. | Refinement is run, allowing a graph to be drawn. | The correct graph is able to be drawn. | \n",
    "| User analysing unrefined data | Testing the user-based analysis error handling functions correctly. | Calling userActivity.py on an unrefined file. | An appropriate error is returned. | The correct error is printed. | \n",
    "| Changing interactive visual values | Testing that interactive visuals can be updated. | Creating an interactive visual and changing its N value. | The visual updates based on the new value. | The visual updates correctly. | \n",
    "| Refining a non-existent file | Testing that the error handling for missing files functions correctly for refinement | Calling refineData.py on a non-existent file | An appropriate error is returned. | The correct error is printed. | \n",
    "| Analysing a non-existent file | Testing that the error handling for missing files functions correctly for data analysis | Calling analyseData.py on a non-existent file and repeating for other analysis executables. | An appropriate error is returned. | The correct error is printed. | \n",
    "| Refining an empty file | Testing that the error handling for empty files functions correctly for refinement | Calling refineData.py on an empty file | An appropriate error is returned. | The correct error is printed. | \n",
    "| Analysing an empty file | Testing that the error handling for empty files functions correctly for data analysis | Calling analyseData.py on an empty file and repeating for other analysis executables. | An appropriate error is returned. | The correct error is printed. | \n",
    "| Saving visuals to a folder | Testing that the saving of visuals functions correctly. | Deleting the images folder and running save_visuals | The directory is created and populated with visuals. | The directory of images was created. | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring MatPlot display settings for showing figures in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Refinement Process\n",
    "\n",
    "The Data Refinement process uses the refineData.py and UnrefinedDataFrame.py files.\n",
    "\n",
    "We created the UnrefinedDataFrame class to allow for easier maintaining of a flag tracking whether the data had changed in the process or not as it would be directly tied to a stored Dataframe.\n",
    "\n",
    "The refinement process was as follows:\n",
    "- Remove any duplicates\n",
    "- Update the headers to more appropriate names\n",
    "- Clean formatting of columns that require a specific data type\n",
    "- Adding columns to store data found from analysis (e.g. whether a row is a Reply or not)\n",
    "- Encoding the text to remove unusual characters.\n",
    "- Replacing empty values with blank data.\n",
    "- If any changes have been made, write the updated data to a file\n",
    "\n",
    "This file also contains helper methods for the following:\n",
    "- Checking if a tweet is a retweet by checking its text \n",
    "- Finding the user being retweeted from a retweet's text\n",
    "- Checking if a tweet is a reply by checking its Reply Username value\n",
    "- Finding the Hashtags in a tweet by checking its Entities value\n",
    "- Finding the Mentions in a tweet by checking its Entities value\n",
    "- Finding the App used for a tweet by checking its Source value\n",
    "- Comparing two Arrays to see if they are the same.\n",
    "\n",
    "### Finding if Changes have been Made\n",
    "\n",
    "To find if changes had been made a combination of checks were used:\n",
    "- For methods that could remove rows, the original frame length was compared to the result to see if it had changed.\n",
    "- For methods that changed or added headers, the original frame's headers are compared to the desired outcome headers to see if changes should be made.\n",
    "\n",
    "These then updated the flag in the class to indicate if the data needed to be saved to a new, updated file after finishing refinement.\n",
    "\n",
    "### Finding if a Tweet is a Retweet or a Reply\n",
    "\n",
    "From manual analysis of the data, we found that all replies contained a value for Reply Username and all retweets began their text with \"RT @\".\n",
    "\n",
    "As a result, we decided the quickest way to find if a tweet is a retweet was to pattern match its text for the retweet text and the quickest way to find if it was a reply was to check if it had a value stored for Reply Username.\n",
    "\n",
    "### Storing Additional Data to be used in Analysis\n",
    "\n",
    "To increase the speed of analysis, the result of checking rows to see if they are a reply, reweet, etc or for values such as hashtags used was saved in additional columns.\n",
    "\n",
    "We decided to do this because these values are accessed multiple times during data analysis methods and so we found calculating them and saving them during refinement was far quicker than calculating them multiple times during analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../code/refineData.py\" \"../data/LargerVariedData.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Analysis Process\n",
    "\n",
    "The Data Analysis process uses the analyseData.py and AnalysedDataFrame.py files.\n",
    "\n",
    "We created the AnalysedDataSet class to allow for easy storage of analysis results and helper methods tied to a specific dataset. \n",
    "This made it easier to generate visuals and output results from as many relevant results were already calculated, thus speeding up the process of making visuals.\n",
    "\n",
    "When an instance of the class is created, any static values are immediately calculated and stored so they can be output.\n",
    "\n",
    "We analysed the following data (any linked to a visual later on will have further justifications explained alongside them):\n",
    "- The numbers of Tweets, Retweets, Replies, Unique Apps and Unique Users in the dataset.\n",
    "- The Avg. number of Tweets, Retweets and Replies made by a User\n",
    "- The Avg. combined number of Tweets, Retweets and Replies sent on an App.\n",
    "- The Avg. number of unique users replying to Users\n",
    "- The Top 10 Hashtags and Apps used\n",
    "- The Top 10 most Replied to Users and Hashtags\n",
    "- The Top 10 most Retweeted Users\n",
    "- The Top 10 trending Users and Hashtags in each Month\n",
    "- The Top 10 Users with the highest ratio of being replied to against replying to others\n",
    "\n",
    "### Storage of Results for Visualisations\n",
    "\n",
    "In order to allow analysis results to be used in later visualisations, we decided to store them in memory under 'data' after the analysis had been run.\n",
    "\n",
    "This allowed it to be accessed throughout the rest of the notebook for relevant methods, meaning analysis only needs to be run once for any number of visuals to be generated.\n",
    "\n",
    "### Analysis of Averages\n",
    "\n",
    "We decided to include further Avg. based analysis to further contextualise both the number of apps and the number of Users making replies in comparison to the data set.\n",
    "\n",
    "### Trending Data Analysis\n",
    "\n",
    "We decided to add analysis of the trends against the month to get a clearer idea of how the interaction between Users and with Hashtags changed over the period of time represented in the data set.\n",
    "\n",
    "### Classification of Tweets\n",
    "\n",
    "In our analysis of the numbers of Tweets, Retweets and Replies, we have chosen Tweets to mean not a Retweet or Reply.\n",
    "\n",
    "Although could be seen as slightly counter intuitive, we felt it was worth including as the combined number can also be seen in the Number of Records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run \"../code/analyseData.py\" \"../data/LargerVariedData_REFINED.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Generators\n",
    "\n",
    "Generating visuals uses the generateVisuals.py file.\n",
    "\n",
    "This loads in a set of helper methods into the notebook which can be called on a refined data frame to produce figures. These methods allow for the following:\n",
    "- Plotting of the Top N Most Replied to Users, Most Retweeted Users, Most Used Apps, Most Replied to Users, Most Used Hashtags and Users with Highest Reply vs Replied To Ratio\n",
    "- Abstract plotting of a bar chart for the Top N Values in a dictionary\n",
    "- Plotting of a Wordcloud of the most frequent hashtags after the Top N\n",
    "- Plotting of the amount of activity on a given date and time\n",
    "- Creation of an interactive visual based on an input N value\n",
    "- Saving of the above visuals to an images directory\n",
    "- Plotting a Pie Chart based on the Top N Apps used\n",
    "\n",
    "These methods can then be called either individually to get the relevant figures or running the save method from a notebook displays all the figures due to the inline display settings.\n",
    "\n",
    "### Adding an Abstract generator for Top N Bar Charts\n",
    "\n",
    "Due to heavy repetition in the process to create a Top N bar chart based on dictionary values, we decided to abstract the process to a singular method which took the data and its header as an input alongside the N value.\n",
    "\n",
    "We then also added individual named methods for each graph as they were required to allow for interactive versions of the graphs to be created abstractly.\n",
    "\n",
    "### Generating Interactive Visuals Abstractly\n",
    "\n",
    "To clean up the repetition in the interactive visual creation, we created a method that could take the aforementioned generator methods (and the generator for the wordcloud) and create a version of the visual with a slider to chose the N value included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../code/generateVisuals.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Hashtags Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, plot_top_hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hashtags after Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, hashtags_wordcloud))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Most Frequent Apps\n",
    "\n",
    "We decided to include this analysis to identify how most Users would access Twitter. \n",
    "\n",
    "From this, you can see whether most users are on mobile, PC, etc.\n",
    "\n",
    "Included in this set of visuals is a Pie chart displaying the proportion of these values by percentage - this includes grouping of the values after the N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, plot_top_apps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, app_pie_chart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Most Frequently Replied to Users\n",
    "\n",
    "We decided to include this analysis to identify which Users caused the most replies.\n",
    "\n",
    "This could then be further investigated to try and draw links between what might cause someone to reply to a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, plot_top_interactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Most Retweeted Users\n",
    "\n",
    "We decided to include this analysis to identify which Users caused the most retweets.\n",
    "\n",
    "This could then be further investigated to try and draw links between what might cause someone to retweet something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, plot_top_rt_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Users with Higest Reply/Replied to Ratio\n",
    "\n",
    "We decided to include this analysis to see which users received the most replies in proportion to the amount of replies they made themselves.\n",
    "\n",
    "This could then be further investigated to see what caused these specific users to receive a large amount of engagement without engaging as much with others themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, plot_top_replied_ratios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top N Most Replied to Hashtags\n",
    "\n",
    "We decided to include this analysis to see which Hashtags received the most replies.\n",
    "\n",
    "This could then be further investigated to see if there is a link between the related topics and whether users are more likely to reply to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(create_interactive_visual(data, plot_top_replied_hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved Version of Visuals - Including Timeline of Activity Graph\n",
    "\n",
    "This generates all of the visuals, using 10 as the default N value and saves them to the images directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_visuals(data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of User Activity\n",
    "\n",
    "The Analysis of User Activity uses the userActivity.py and AnalyseUserActivity.py files.\n",
    "\n",
    "These function similarly to the files used for general analysis, but instead filter the data to look at a specific user's tweets only.\n",
    "\n",
    "We chose to do this as it allowed us to plot the pattern of a User's tweets, replies and retweets over the time found in the data set.\n",
    "\n",
    "It also allowed for a specific user's number of Followers, Friends, Tweets, Retweets and Replies to be analysed.\n",
    "\n",
    "The saving of the visual is handled in its displaying method.\n",
    "\n",
    "\n",
    "### Generation of Tweet, Retweet and Reply Count Lists\n",
    "\n",
    "In order to store the counts of each type of tweet against time, we decided to create an array for each that had values corresponding to the time at each row in the data set.\n",
    "\n",
    "By passing these to the plotting function, this allowed for all 3 counts to be plotted on one graph against time which makes it easier to compare them and find patterns in a User's activity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../code/userActivity.py\" \"../data/LargerVariedData_REFINED.csv\" \"BBCNews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data.output_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Geographical Data\n",
    "\n",
    "The Analysis of Geographical Data uses the Geomap.py and GeomapRun.py files.\n",
    "\n",
    "We added this analysis to better contextualise the Coordinate data stored alongside some tweets. Additionally, one of our group members was curious as to whether anyone from his country (32.4279° N, 53.6880° E) had engaged with this topic - much to his disappointment, nobody had.\n",
    "\n",
    "The resulting data could be used to further investigate which countries were most engaged with the topic and why this might be.\n",
    "\n",
    "To demonstrate the results of the analysis a map with markers on each coordinate was plotted, a heat map was generated and a list of top n countries in terms of number of tweets was generated.\n",
    "\n",
    "To make the demonstrations more detailed and to be able to map a specific coordinate to a country a third party map API (TomTom) was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../code/GeomapRun.py\" \"../data/LargerVariedData_REFINED.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data.top_n_countries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "def GAnalysis(n):\n",
    "    map_data.print_data(n)\n",
    "display(interactive(GAnalysis, n=widgets.IntSlider(min=1, max=195, step=1, value=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map of Counts of Tweet by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_data.show_markup_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmap of Activity by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "map_data.show_heat_map()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis of User Interactions\n",
    "\n",
    "The Network Analysis of User Interactions uses the Networks.py and networkAnalysis.py files.\n",
    "\n",
    "To better demonstrate the interactions between users regarding this event, a “NetworkX” graph was drawn. \n",
    "\n",
    "In this graph nodes represent users (Using their usernames), and the edges represent who they replied to (in red), who they retweeted from (in blue) and who they mentioned (green). \n",
    "\n",
    "As this task is computationally expensive a random sample is taken from the main dataset. The sample size can be set by the user. \n",
    "\n",
    "It is advised not to enter any sample size larger than 1500 as the time it takes the computer to plot the graph would increase substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "def NAnalysis(n):\n",
    "    network_data.displayNetwork(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run \"../code/networkAnalysis.py\" \"../data/LargerVariedData_REFINED.csv\"\n",
    "display(interactive(NAnalysis, n=widgets.IntSlider(min=100, max=3000, step=100, value=500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We managed to implement a data refinement, analysis and visualisation system, including graphs, wordclouds and maps. This also included validation for inputs and unit tests to ensure the refinement process is valid.\n",
    "\n",
    "Overall, we view this submission as a large success due to the lack of errors being output and the large variety of analysis present.\n",
    "\n",
    "Given more time, we would have liked to implement more forms of interactivity into the visualisations such as changing colours or allowing for inspection of the values stored in different columns.\n",
    "\n",
    "We also would have liked to update our parsing of the Entities column to use JSON parsing. Initially we did not as we thought we only needed the Hashtags data from it but as our program has expanded, we’ve needed to parse it multiple times. As a result, we think a JSON parser could be more efficient but we did not have time to implement and test it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Provenance & Breakdown of Functionality\n",
    "\n",
    "The following code was written directly by us:\n",
    "- refineData.py file:\n",
    "    - Contains a main method which takes the filepath to a CSV file as input to refine the data and outputs a new file with the refined data \n",
    "- UnrefinedDataFrame.py file:\n",
    "    - Contains a class to encapsulate the process of reading in a CSV dataset and running required refinements\n",
    "    - Also contains helper methods to help in this process\n",
    "- analyseData.py file:\n",
    "    - Contains a main method which takes the refined CSV file as input to run an analysis on it and outputs the results\n",
    "    - Also contains helper methods to help in this process\n",
    "- analyseDataSet.py file: \n",
    "    - Contains a class to perform the actual analysis of the dataset\n",
    "- generateVisuals.py file:\n",
    "    - Contains a class with a number of helper methods that takes an analysed dataset and uses it to create figures and for them to be saved in a directory\n",
    "- networksAnalysis.py file:\n",
    "    - Contains a main method which takes as input the filepath to the refined CSV file and so that a network analysis can be done\n",
    "- Network.py file:\n",
    "    - Contains a class which performs the actual network analysis which is done in the constructor method\n",
    "    - Also contains a method to display and save the resulting graph\n",
    "- userActivity.py file:\n",
    "    - Contains a main method which takes as input the filepath to the refined CSV file and the username of the intended user\n",
    "- AnalyseUserActivity.py file:\n",
    "    - Contains a class which performs an analysis on the activity of the given user.\n",
    "    - Also contains a method to display and save the resulting graph\n",
    "- Test.py file\n",
    "    - Contains unit tests for the auxillary methods used in UnrefinedDataFrame.py\n",
    "- Geomap.py file\n",
    "    - Contains a class which performs the geographical analysis\n",
    "    - Also includes helper methods to output and save the results.\n",
    "- GeomapRun.py file\n",
    "    - Contains a main method which takes as input the filepath to the refiend CSV file so that geographical analysis can be done\n",
    "\n",
    "This program doesn’t build upon any source code as there isn’t any provided.\n",
    "\n",
    "The following code was built upon or inspired by externally sourced files.\n",
    "- Creating the base map using the map provided by tom-tom API found at: Sandy_4242 (2018). Display TomTom map with folium. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/53415977/display-tomtom-map-with-folium [Accessed 8 Apr. 2022]\n",
    "- Callback data used to modify the look of the markers on the map found at: OgnjanD (2018). Adding text to Folium FastMarkerCluster markers? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/50661316/adding-text-to-folium-fastmarkercluster-markers [Accessed 8 Apr. 2022].\n",
    "- NetworkX implementation found at: Networkx.org. (2022). Tutorial — NetworkX 2.7.1 documentation. [online] Available at: https://networkx.org/documentation/stable/tutorial.html [Accessed 8 Apr. 2022].\n",
    "- Colour coding the graph: timeislove (2014). networkx - change color/width according to edge attributes - inconsistent result. [online] Stack Overflow. Available at: https://stackoverflow.com/questions/25639169/networkx-change-color-width-according-to-edge-attributes-inconsistent-result [Accessed 8 Apr. 2022].\n",
    "- Folium map implementation found at: studres.cs.st-andrews.ac.uk. (n.d.). Web Login Service - Loading Session Information. [online] Available at: https://studres.cs.st-andrews.ac.uk/CS2006/Lectures/Python/W6-15_Maps/W6-15_Maps.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "1-\tPeixoto, P. (2020). Performance Comparison - graph-tool: Efficent network analysis with python. [online] Skewed.de. Available at: https://graph-tool.skewed.de/performance [Accessed 8 Apr. 2022].\n",
    "\n",
    "3- Sullivan, C. (2015). How to monitor status of networkx graph creation? [online] Stack Overflow. Available at: https://stackoverflow.com/questions/34170634/how-to-monitor-status-of-networkx-graph-creation [Accessed 7 Apr. 2022].\n",
    "\n",
    "2- Stack Overflow. (2008). Visualizing Undirected Graph That’s Too Large for GraphViz? [online] Available at: https://stackoverflow.com/questions/238724/visualizing-undirected-graph-thats-too-large-for-graphviz#comment102544_238724 [Accessed 7 Apr. 2022].\n",
    "\n",
    "4- TomTom (2021). Geocoding quota per minute. [online] TomTom Maps APIs community. Available at: https://devforum.tomtom.com/t/geocoding-quota-per-minute/1538 [Accessed 8 Apr. 2022].\n",
    "\n",
    "5- Tomtom.com. (2022). Pricing. [online] Available at: https://developer.tomtom.com/store/maps-api#:~:text=We%20offer%2050%2C000%20free%20tile,every%20step%20of%20the%20way. [Accessed 8 Apr. 2022]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
